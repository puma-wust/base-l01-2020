{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Matrix factorization\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix factorization is a technique used to decompose a matrix into two smaller matrices, so that the multiplication of these matrices is equal to the original matrix.\n",
    "\n",
    "One of the popular applications of this technique are recommender systems, where MF is used to extract common hidden features shared between each user (first axis of the matrix) and item to be recommended (second axis), which explain the interactions between them. Those features need to be inferred from the data. This approach is described wider in [the article](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf).\n",
    "\n",
    "![image.png](https://miro.medium.com/max/3378/0*k0m45RKFE7YN3UHf.png)\n",
    "\n",
    "The original matrix shape is $m \\times n$. The result should be two matrices:\n",
    "  *  of shape $m \\times d$\n",
    "  *  of shape $d \\times n$\n",
    "where $d$ is chosen to compromise on computation efficiency and number of features required to represent the data.\n",
    "\n",
    "The value $\\hat{r}_{ui}$ from the original matrix (originally - a matrix of ratings of items) is calculated as the dot product of a single row from the first matrix $p_u$ (vector representing a user) and a single column of the second one $q_i$ (vector representing an item).\n",
    "\n",
    "$$\\hat{r}_{ui} = q_i^T p_u$$\n",
    "\n",
    "To learn the factor vectors $\\mathbf{p}$ and $\\mathbf{q}$ one can use for example Mean Squared Error:\n",
    "\n",
    "$$MSE = \\sum_{(u, i)\\in \\mathcal{K}} \\left(r_{ui} - q_i^T p_u\\right)^2$$\n",
    "\n",
    "where $\\mathcal{K}$ is the set of $(u, i)$ pairs, for which the true $r_{ui}$ is known.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To avoid overfitting, various **regularizations** can be used, such as **L2 regularization** (*ridge regression*):\n",
    "\n",
    "$$L2 = \\sum_{j=0}^p \\beta_j^2$$\n",
    "\n",
    "The L2 term, added with coefficient $\\lambda$ to the loss function encourages the params $\\beta_j$ to be small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Homework\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Create a class inheriting from `torch.nn.Module`. Declare $\\mathbf{p}$ and $\\mathbf{q}$ matrices and set their sizes according to the passed params. Define `forward(self)` method to return a dot product of the matrices.\n",
    "2. Create a basic loss function (which will be , according to the equation:\n",
    "$$L = MSE + \\lambda \\cdot L2$$\n",
    "   You might implement your own functions or use the PyTorch builtins.\n",
    "   > *NOTE*: most PyTorch optimizers already include L2 regularization, controlled by `weight_decay` param, but we will not be using it here.\n",
    "3. Create a function `train` that will train the model for a given number of epochs, with a given learning rate. Enable passing the source matrix and the model. Train the model on matrix given at the start, with given params. Plot the loss function.\n",
    "4. Expand the loss function and add additional constraints:\n",
    "   *  apply L1 regularization\n",
    "   $$L1 = \\sum_{j=0}^p \\left|\\beta_j\\right|$$\n",
    "   *  encourage the length of each vector (i.e. per one person or item) to be 1\n",
    "   $$L_{unit} = \\sum_{u \\in \\mathcal{K}} \\left| |p_u| - 1\\right| + \\sum_{i \\in \\mathcal{K}} \\left|\\left|q_i\\right| - 1\\right|$$\n",
    "   \n",
    "   Verify their influence on the achieved loss.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 1\"\"\"\n",
    "\n",
    "class MatrixFactorizer(torch.nn.Module):\n",
    "    def __init__(self, dim_h, dim_w, n_features):\n",
    "        \"\"\" Initialize the model.\n",
    "        \n",
    "        :param dim_h: number of rows in \"user\" matrix\n",
    "        :param dim_w: number of columns in \"item\" matrix\n",
    "        :param n_features: number of features in both matrices\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_mf():\n",
    "    params = {(3, 5, 1): (3, 5),\n",
    "              (7, 7, 1): (7, 7),\n",
    "              (49, 35, 3): (49, 35)}\n",
    "    for (dim_h, dim_w, n_features), (result_h, result_w) in params.items():\n",
    "        mf = MatrixFactorizer(dim_h, dim_w, n_features)\n",
    "        assert (mf().size() == (result_h, result_w))\n",
    "\n",
    "# Check if MF has learnable parameters\n",
    "def test_mf_parameters():\n",
    "    sizes = [(49, 3), (3, 35)]\n",
    "    mf = MatrixFactorizer(49, 35, 3)\n",
    "    for idx, parameter in enumerate(mf.parameters()):\n",
    "        assert parameter.size() == sizes[idx]\n",
    "\n",
    "test_mf()\n",
    "test_mf_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 2\"\"\"\n",
    "def loss_fn(r, factorizer, rate=0.01):\n",
    "    \"\"\" Calculate the loss.\n",
    "    \n",
    "    :param r: the original matrix\n",
    "    :param mf: the matrix factorizer model, initialized\n",
    "    :param rate: l2 regularization term rate\n",
    "    :return: loss function value\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def l2_norm(params):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_l2():\n",
    "    params = {(3.,): 9.00,\n",
    "              (0.4, 0.6): 0.52,\n",
    "              ((0.1, 0.2), (0.4, 0.5)): 0.46}\n",
    "    for data, result in params.items():\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        result = torch.tensor(result, dtype=torch.float)\n",
    "        l2 = torch.round(\n",
    "            100 * l2_norm(data)\n",
    "        ) / 100\n",
    "        assert l2 == result\n",
    "        \n",
    "test_l2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 3\"\"\"\n",
    "def train(model, data,\n",
    "          num_epochs=10_000,\n",
    "          lr=1e-3,\n",
    "          loss_fn=loss_fn):\n",
    "    \"\"\" Train MF model.\n",
    "    \n",
    "    :param model: MatrixFactorizer model (initialized)\n",
    "    :param data: matrix to be factorized\n",
    "    :param num_epochs: number of epochs to perform\n",
    "    :param lr: used learning rate\n",
    "    :param loss: loss function (callable)\n",
    "    :return: loss history\n",
    "    \"\"\"\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Task 4\"\"\"\n",
    "def l1_norm(params):\n",
    "    pass\n",
    "\n",
    "def length_reg(params):\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_l1():\n",
    "    params = {(3.,): 3.00,\n",
    "              (0.4, 0.6): 1.0,\n",
    "              (0.1, 0.2, 0.4, 0.5): 1.2}\n",
    "    for data, result in params.items():\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        result = torch.tensor(result, dtype=torch.float)\n",
    "        l1 = torch.round(\n",
    "            100 * l1_norm(data)\n",
    "        ) / 100\n",
    "        assert l1 == result\n",
    "        \n",
    "test_l1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_length_reg():\n",
    "    params = {((3.,),): 2.00,\n",
    "              ((0.4, 0.6),): 0.28,\n",
    "              ((0.1, 0.2), (0.4, 0.5)): 1.14}\n",
    "    for data, result in params.items():\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        result = torch.tensor(result, dtype=torch.float)\n",
    "        reg = torch.round(\n",
    "            100 * length_reg(data)\n",
    "        ) / 100\n",
    "        assert reg == result\n",
    "\n",
    "test_length_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn_reg(r, factorizer, l1_rate=0., l2_rate=0.4, length_rate = 0.2):\n",
    "    \"\"\" Calculate the loss.\n",
    "    \n",
    "    :param r: the original matrix\n",
    "    :param mf: the matrix factorizer model, initialized\n",
    "    :param l1_rate: l1 regularization term rate\n",
    "    :param l2_rate: l2 regularization term rate\n",
    "    :param length_rate: length regularization term rate\n",
    "    :return: loss function value\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
